% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{Shah2024BiasDeepLearningReview,
  title={A comprehensive review of bias in deep learning models: Methods, impacts, and future directions},
  author={Shah, Milind and Sureja, Nitesh},
  journal={Archives of Computational Methods in Engineering},
  volume={32},
  number={1},
  pages={255--267},
  year={2025},
  publisher={Springer}
}

@article{Gallegos2024BiasFairnessLLMs,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  volume={50},
  number={3},
  pages={1097--1179},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{Lei2022BiasSentenceDiscourse,
  title={Sentence-level media bias analysis informed by discourse structures},
  author={Lei, Yuanyuan and Huang, Ruihong and Wang, Lu and Beauchamp, Nick},
  booktitle={Proceedings of the 2022 conference on empirical methods in natural language processing},
  pages={10040--10050},
  year={2022}
}
@inproceedings{DecodingNewsBias2025,
  title={Decoding News Bias: Multi Bias Detection in News Articles},
  author={Shah, Bhushan Santosh and Shah, Deven Santosh and Attar, Vahida},
  booktitle={Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
  pages={97--104},
  year={2024}
}

@article{bang2024measuringpoliticalbiaslarge,
  title={Measuring political bias in large language models: What is said and how it is said},
  author={Bang, Yejin and Chen, Delong and Lee, Nayeon and Fung, Pascale},
  journal={arXiv preprint arXiv:2403.18932},
  year={2024}
}

@article{lin2024investigatingbiasllmbasedbias,
  title={Investigating bias in llm-based bias detection: Disparities between llms and human perception},
  author={Lin, Luyang and Wang, Lingzhi and Guo, Jinsong and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2403.14896},
  year={2024}
}

@article{baly2020we,
  title={We can detect your bias: Predicting the political ideology of news articles},
  author={Baly, Ramy and Martino, Giovanni Da San and Glass, James and Nakov, Preslav},
  journal={arXiv preprint arXiv:2010.05338},
  year={2020}
}

@inproceedings{semeval2019hyperpartisan,
  title={SemEval-2019 task 4: Hyperpartisan news detection},
  author={Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
  booktitle={Proceedings of the 13th international workshop on semantic evaluation},
  pages={829--839},
  year={2019}
}

@article{spinde2021babe,
  title={Neural Media Bias Detection Using Distant Supervision With BABE--Bias Annotations By Experts},
  author={Spinde, Timo and Plank, Manuel and Krieger, Jan-David and Ruas, Terry and Gipp, Bela and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2209.14557},
  year={2022}
}

@inproceedings{spinde2021mbic,
title = {MBIC – A Media Bias Annotation Dataset
Including Annotator Characteristics},
booktitle = {Proceedings of the iConference 2021},
author = {Spinde, Timo and Rudnitckaia, Lada and Sinha,
Kanishka and Hamborg, Felix and Gipp, Bela and
Donnay, Karsten},
year = {2021},
location = {Beijing, China (Virtual Event)},
month = {March},
topic = {newsanalysis}
}

@inproceedings{chen2018flipbias,
  title={Learning to Flip the Bias of News Headlines},
  author="Chen, Wei-Fan  and
      Wachsmuth, Henning  and
      Al-Khatib, Khalid  and
      Stein, Benno",
booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    url = "https://aclanthology.org/W18-6509/",
    doi = "10.18653/v1/W18-6509",
    pages = "79--88"
}

@inproceedings{fan2019basil,
    title = "In Plain Sight: Media Bias Through the Lens of Factual Reporting",
    author = "Fan, Lisa  and
      White, Marshall  and
      Sharma, Eva  and
      Su, Ruisi  and
      Choubey, Prafulla Kumar  and
      Huang, Ruihong  and
      Wang, Lu",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1664/",
    doi = "10.18653/v1/D19-1664",
    pages = "6343--6349"
}


@inproceedings{recasens2013weasel,
  title={Linguistic models for analyzing and detecting biased language},
  author={Recasens, Marta and Danescu-Niculescu-Mizil, Cristian and Jurafsky, Dan},
  booktitle={Proceedings of the 51st annual meeting of the Association for Computational Linguistics (volume 1: long papers)},
  pages={1650--1659},
  year={2013}
}

@inproceedings{hube2018detecting,
  title={Detecting biased statements in wikipedia},
  author={Hube, Christoph and Fetahu, Besnik},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1779--1786},
  year={2018}
}

@inproceedings{devlin2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}
}

@inproceedings{lewis2020bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703/",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880"
}

@inproceedings{krieger2022daroberta,
  title={A domain-adaptive pre-training approach for language bias detection in news},
  author={Krieger, Jan-David and Spinde, Timo and Ruas, Terry and Kulshrestha, Juhi and Gipp, Bela},
  booktitle={Proceedings of the 22nd ACM/IEEE joint conference on digital libraries},
  pages={1--7},
  year={2022}
}

@article{lin2024inditag,
  title={Inditag: An online media bias analysis and annotation system using fine-grained bias indicators},
  author={Lin, Luyang and Wang, Lingzhi and Guo, Jinsong and Li, Jing and Wong, Kam-Fai},
  journal={arXiv preprint arXiv:2403.13446},
  year={2024}
}

@inproceedings{farr2024llmchain,
  title={Llm chain ensembles for scalable and accurate data annotation},
  author={Farr, David and Manzonelli, Nico and Cruickshank, Iain and Starbird, Kate and West, Jevin},
  booktitle={2024 IEEE International Conference on Big Data (BigData)},
  pages={2110--2118},
  year={2024},
  organization={IEEE}
}

@inproceedings{hartvigsen2022toxicity,
author = {Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
title = {ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection},
booktitle = {ACL 2022},
year = {2022},
month = {May},
url = {https://www.microsoft.com/en-us/research/publication/toxigen-a-large-scale-machine-generated-dataset-for-adversarial-and-implicit-hate-speech-detection/}
}

@article{ronnback2025biasoutlets,
    doi = {10.1371/journal.pone.0321418},
    author = {Rönnback, Ronja AND Emmery, Chris AND Brighton, Henry},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Automatic large-scale political bias detection of news outlets},
    year = {2025},
    month = {05},
    volume = {20},
    url = {https://doi.org/10.1371/journal.pone.0321418},
    pages = {1-23},
    number = {5}
}

@inproceedings{liu2022politics,
    title = "{POLITICS}: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection",
    author = "Liu, Yujian  and
      Zhang, Xinliang Frederick  and
      Wegsman, David  and
      Beauchamp, Nicholas  and
      Wang, Lu",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.101/",
    doi = "10.18653/v1/2022.findings-naacl.101",
    pages = "1354--1374"
}

@article{wang2020kepler,
  author       = {Xiaozhi Wang and
                  Tianyu Gao and
                  Zhaocheng Zhu and
                  Zhiyuan Liu and
                  Juanzi Li and
                  Jian Tang},
  title        = {{KEPLER:} {A} Unified Model for Knowledge Embedding and Pre-trained
                  Language Representation},
  journal      = {CoRR},
  volume       = {abs/1911.06136},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.06136},
  eprinttype    = {arXiv},
  eprint       = {1911.06136},
  timestamp    = {Thu, 17 Oct 2024 17:26:40 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-06136.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS {ridnik2021asymmetric,
author = { Ridnik, Tal and Ben-Baruch, Emanuel and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi },
booktitle = { 2021 IEEE/CVF International Conference on Computer Vision (ICCV) },
title = {{ Asymmetric Loss For Multi-Label Classification }},
year = {2021},
volume = {},
ISSN = {},
pages = {82-91},
abstract = { In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL. },
keywords = {Training;Computer vision;Adaptive systems;Object detection;Benchmark testing;Complexity theory;Task analysis},
doi = {10.1109/ICCV48922.2021.00015},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV48922.2021.00015},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Oct}

@misc{yang2024largelanguagemodelsoptimizers,
      title={Large Language Models as Optimizers}, 
      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
      year={2024},
      eprint={2309.03409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.03409}, 
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@misc{brown2020gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}
@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@misc{latif2023finetuningchatgptautomaticscoring,
      title={Fine-tuning ChatGPT for Automatic Scoring}, 
      author={Ehsan Latif and Xiaoming Zhai},
      year={2023},
      eprint={2310.10072},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.10072}, 
}
@INPROCEEDINGS{Bousselham2024FineTuningGPT,
  author={Bousselham, Hamza and Nfaoui, El Habib and Mourhir, Asmaa},
  booktitle={2024 International Conference on Computer, Electrical & Communication Engineering (ICCECE)}, 
  title={Fine-Tuning GPT on Biomedical NLP Tasks: An Empirical Evaluation}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Adaptation models;Technological innovation;Biological system modeling;Predictive models;Propulsion;Parallel processing;Transformers;Large Language Models;GPT-3;Fine-tuning;NLP;Embeddings;Pretrained models},
  doi={10.1109/ICCECE58645.2024.10497313}}

@misc{ibrahim2024analyzingpoliticalstancestwitter,
      title={Analyzing political stances on Twitter in the lead-up to the 2024 U.S. election}, 
      author={Hazem Ibrahim and Farhan Khan and Hend Alabdouli and Maryam Almatrooshi and Tran Nguyen and Talal Rahwan and Yasir Zaki},
      year={2024},
      eprint={2412.02712},
      archivePrefix={arXiv},
      primaryClass={cs.SI},
      url={https://arxiv.org/abs/2412.02712}, 
}

@article{Wen_2023,
   title={ChatGPT v.s. media bias: A comparative study of GPT-3.5 and fine-tuned language models},
   volume={21},
   ISSN={2755-273X},
   url={http://dx.doi.org/10.54254/2755-2721/21/20231153},
   DOI={10.54254/2755-2721/21/20231153},
   number={1},
   journal={Applied and Computational Engineering},
   publisher={EWA Publishing},
   author={Wen, Zehao and Younes, Rabih},
   year={2023},
   month=oct, pages={249–257} }

@INPROCEEDINGS{hamborg2019automatedmediabias,
  author={Hamborg, Felix and Zhukova, Anastasia and Gipp, Bela},
  booktitle={2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Automated Identification of Media Bias by Word Choice and Labeling in News Articles}, 
  year={2019},
  volume={},
  number={},
  pages={196-205},
  keywords={Encoding;Media;Semantics;Training;Social sciences;Labeling;Biological system modeling;news slant;news bias;automated content analysis;automated frame analysis;entity perception;emotions;CAS;CAQDAS;NLP},
  doi={10.1109/JCDL.2019.00036}}

@misc{rashkin2016connotationframesdatadriveninvestigation,
      title={Connotation Frames: A Data-Driven Investigation}, 
      author={Hannah Rashkin and Sameer Singh and Yejin Choi},
      year={2016},
      eprint={1506.02739},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1506.02739}, 
}

@Article{mendhakar2022linguisticprofiling,
AUTHOR = {Mendhakar, Akshay},
TITLE = {Linguistic Profiling of Text Genres: An Exploration of Fictional vs. Non-Fictional Texts},
JOURNAL = {Information},
VOLUME = {13},
YEAR = {2022},
NUMBER = {8},
ARTICLE-NUMBER = {357},
URL = {https://www.mdpi.com/2078-2489/13/8/357},
ISSN = {2078-2489},
ABSTRACT = {Texts are composed for multiple audiences and for numerous purposes. Each form of text follows a set of guidelines and structure to serve the purpose of writing. A common way of grouping texts is into text types. Describing these text types in terms of their linguistic characteristics is called ‘linguistic profiling of texts’. In this paper, we highlight the linguistic features that characterize a text type. The findings of the present study highlight the importance of parts of speech distribution and tenses as the most important microscopic linguistic characteristics of the text. Additionally, we demonstrate the importance of other linguistic characteristics of texts and their relative importance (top 25th, 50th and 75th percentile) in linguistic profiling. The results are discussed with the use case of genre and subgenre classifications with classification accuracies of 89 and 73 percentile, respectively.},
DOI = {10.3390/info13080357}
}

@inproceedings{Recasens2013LinguisticMF,
  title={Linguistic Models for Analyzing and Detecting Biased Language},
  author={Marta Recasens and Cristian Danescu-Niculescu-Mizil and Dan Jurafsky},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:2772094}
}


@inproceedings{darwish2020aaai,
title = "Unsupervised user stance detection on Twitter",
abstract = "We present a highly effective unsupervised framework for detecting the stance of prolific Twitter users with respect to controversial topics. In particular, we use dimensionality reduction to project users onto a low-dimensional space, followed by clustering, which allows us to find core users that are representative of the different stances. Our framework has three major advantages over pre-existing methods, which are based on supervised or semi-supervised classification. First, we do not require any prior labeling of users: instead, we create clusters, which are much easier to label manually afterwards, e.g., in a matter of seconds or minutes instead of hours. Second, there is no need for domain- or topic-level knowledge either to specify the relevant stances (labels) or to conduct the actual labeling. Third, our framework is robust in the face of data skewness, e.g., when some users or some stances have greater representation in the data. We experiment with different combinations of user similarity features, dataset sizes, dimensionality reduction methods, and clustering algorithms to ascertain the most effective and most computationally efficient combinations across three different datasets (in English and Turkish). We further verified our results on additional tweet sets covering six different controversial topics. Our best combination in terms of effectiveness and efficiency uses retweeted accounts as features, UMAP for dimensionality reduction, and Mean Shift for clustering, and yields a small number of high-quality user clusters, typically just 2- 3, with more than 98\% purity. The resulting user clusters can be used to train downstream classifiers. Moreover, our framework is robust to variations in the hyper-parameter values and also with respect to random initialization.",
author = "Kareem Darwish and Peter Stefanov and Michael Aupetit and Preslav Nakov",
note = "Publisher Copyright: Copyright {\textcopyright} 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.; 14th International AAAI Conference on Web and Social Media, ICWSM 2020 ; Conference date: 08-06-2020 Through 11-06-2020",
year = "2020",
language = "English",
series = "Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM 2020",
publisher = "AAAI Press",
pages = "141--152",
booktitle = "Proceedings of the 14th International AAAI Conference on Web and Social Media, ICWSM 2020",

}
@inproceedings{menzner2025biasscanner,
author = {Menzner, Tim and Leidner, Jochen L.},
title = {BiasScanner: Automatic News Bias Classification for Strengthening Democracy},
year = {2025},
isbn = {978-3-031-88719-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-88720-8_18},
doi = {10.1007/978-3-031-88720-8_18},
abstract = {The increasing consumption of news online in the 21st century coincided with increased publication of disinformation, biased reporting, hate speech and other unwanted Web content.We describe BiasScanner, an application that aims to strengthen democracy by supporting news consumers with scrutinizing news articles they are reading online. BiasScanner contains a server-side pre-trained large language model to identify biased sentences of news articles and a front-end Web browser plug-in. BiasScanner can identify&nbsp;and classify more than two dozen types of media bias at the sentence level, making it the most fine-grained model and only automatic application deployed as a browser plug-in. One special feature is the high-quality, LLM-generated explanations of the model’s decisions.While prior research has addressed news bias detection, we are not aware of any automatic work that resulted in a deployed browser plug-in (c.f. also biasscanner.org for a Web demo).},
booktitle = {Advances in Information Retrieval: 47th European Conference on Information Retrieval, ECIR 2025, Lucca, Italy, April 6–10, 2025, Proceedings, Part V},
pages = {105–110},
numpages = {6},
keywords = {news bias identification, media bias classification, content quality, news analytics, media monitoring, Web applications, natural language processing, information retrieval, information access systems},
location = {Lucca, Italy}
}

@inproceedings{baly2020acl,
author={Baly, Ramy and Karadzhov, Georgi and An, Jisun and Kwak, Haewoon
 and Dinkov,Yoan and Ali, Ahmed and Glass, James and Nakov, Preslav},
year={2020},
title={What was written vs. who read it: News media profiling using text analysis and social media context},
booktitle={Proceedings of ACL 2020}}
