\section{Human Annotation and Baseline Studies}

The establishment of human performance baselines represents a critical component of bias detection research, yet comprehensive studies remain limited. The Media Bias Identification Corpus (MBIC) by \citet{spinde2021mbic} utilized crowdworkers to identify biased language, achieving only slight to fair inter-annotator agreement (Fleiss's Kappa $\approx 0.21$). In response to these annotation challenges, the Bias Annotations by Experts (BABE) dataset \citep{spinde2021babe} employed expert annotators specifically trained to recognize media bias, resulting in improved but still modest inter-annotator reliability (Krippendorff's Alpha $\approx 0.39$).

These findings suggest that even expert human annotators face considerable challenges in consistently identifying biased content. Moreover, these prior studies primarily focused on bias detection rather than directional classification. This thesis addresses this gap by introducing an informal layman baseline study for political ideology prediction that explicitly distinguish ideological direction.
