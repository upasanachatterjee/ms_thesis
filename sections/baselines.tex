\chapter{Experimental Baselines}
\label{sec:baselines}

This chapter establishes baseline performance of three distinct classes of systems on the AllSides political bias classification task: human annotators, fine-tuned transformer architectures, and large language models in zero-shot configuration. Our transformer experiments design addresses the challenge of source leakage by employing a data split where all articles from the same source are contained within a single partition. As no fine-tuning is performed for LLMs, they are evaluated only on this source-controlled split to ensure consistency, and the human baseline study is conducted on a subset of this split as well.

\section{Experimental Setup and Data Partitioning}
\label{sec:experimental-setup}

The AllSides dataset provides two distinct partitioning strategies that enable systematic evaluation of source leakage effects in political bias classification:

\begin{itemize}
    \item \textbf{Media Split}: All articles from the same publication source are assigned to the same partition (train, validation, or test). This configuration ensures that models cannot exploit source-specific stylistic or lexical patterns during evaluation.
    
    \item \textbf{Random Split}: Articles from any source may appear across all partitions, allowing systems to leverage source-identifying information during training and potentially memorize source-specific biases rather than learning generalizable content patterns.
\end{itemize}

The Media Split represents our primary evaluation configuration, as it provides the most stringent test of content-based political bias detection capabilities. The Random Split serves as a diagnostic tool for quantifying the extent to which models rely on source leakage versus content analysis.

\begin{table*}
\centering
\footnotesize
\begin{tabular}{@{}l|cccc|cccc@{}}
\toprule
\textbf{System} & \multicolumn{4}{c|}{\textbf{AllSides Media Split}} & \multicolumn{4}{c}{\textbf{AllSides Random Split}} \\ 
 & F1-Macro & Left & Center & Right & F1-Macro & Left & Center & Right \\
 \midrule
\textbf{Human Performance} & 41.94 & 37.31 & 39.09 & 49.41 & - & - & - & -\\
 \midrule
\textbf{Fine-tuned Transformers} & & & & & & & & \\
BERT & 38.26 & 49.95 & 14.74 & 50.09 & 77.38 & 74.66 & 84.33 & 75.12 \\
BART & 36.01 & 46.92 & 11.31 & 49.80  & 87.93 & 89.38 & 84.98 & 89.44 \\
RoBERTa & 42.34 & 56.82 & 04.47 & 65.73  & 83.49 & 81.39 & 83.98 & 85.11 \\
POLITICS & \textbf{57.66} & \textbf{65.71} & 37.92 & 69.35 &  \textbf{88.31} & \textbf{85.82} & \textbf{91.47} & \textbf{88.44} \\
\midrule
\textbf{Zero-shot LLMs (GPT Family)} & & & & & & & & \\
GPT-4o-mini & 51.86 & 52.88 & 39.76 & 62.96 & - & - & - & -  \\
GPT-5-nano & 50.99 & 44.89 & 49.61 & 58.48 & - & - & - & - \\
o3-mini & 56.92 & 46.54 & \textbf{53.46} & \textbf{70.78} & - & - & - & - \\
\midrule
\textbf{Zero-shot LLMs (LLaMA Family)} & & & & & & & & \\
LLaMA-3.1-8B-instant & 29.01 & 41.44 & 30.29 & 15.30 & - & - & - & - \\
LLaMA-3.3-70B-versatile & 26.01 & 57.26 & 09.86 & 10.93  & - & - & - & - \\ 
\bottomrule
\end{tabular}
\caption{Baseline system performance on AllSides political bias classification.}
\label{tab:main_baseline}
\end{table*}

\section{Human Performance Baseline}
\label{sec:human-baseline}

To establish empirical bounds on the difficulty of article-level political bias classification, we conducted a human annotation study. This baseline offers insights into the cognitive challenges faced by layman annotators in this domain.

\subsection{Annotation Protocol}

We recruited four annotators with demonstrated familiarity with U.S. political discourse and comparable demographic profiles (age under 35, self-identified political orientation ranging from liberal to moderate). To minimize source bias effects, annotators were provided with article content only, with all source attribution and author information removed.

The annotation study employed a random sample of 102 articles from the AllSides Media test split, ensuring balanced representation across ideological categories (34 articles per class: left, center, right). Each article was independently classified by all four annotators using the same three-point ideological scale employed in the original dataset.

\subsection{Human Performance Results}

Human performance proved modest across all metrics, with an average F1-Macro score of 41.94 and individual annotator performance ranging from 35.46 to 46.57. Inter-annotator agreement analysis revealed low agreement (Krippendorff's $\alpha = 0.241$), indicating significant subjectivity in political bias perception even among demographically similar annotators.

Performance varied notably across ideological categories, with annotators achieving higher accuracy on right-leaning content (F1 = 49.41) compared to center (F1 = 39.09) and left-leaning (F1 = 37.31) articles. Post-annotation interviews revealed that annotators experienced particular difficulty with non-hyperpartisan content and expressed that access to source information would have substantially increased their classification confidence.

These results highlight the fundamental challenge of article-level political bias classification and provide important context for interpreting automated system performance. Complete human performance metrics and inter-annotator agreement analysis are presented in Appendix~\ref{sec:appendix-human-tests}.


\section{Fine-tuned Transformer Architectures}
\label{sec:transformer-baselines}

Our transformer baseline evaluation encompasses both general-purpose pre-trained language models and domain-adapted architectures specifically designed for political text analysis. This comparison enables assessment of the value of domain-specific adaptation for political bias detection.

\subsection{General-Purpose Transformer Models}

We evaluated three representative pre-trained transformer architectures: BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019roberta}, and BART \citep{lewis2020bart}. Each model was fine-tuned end-to-end for three-class political bias classification by adding a linear classification head to the pre-trained encoder representations.

All models employed identical training procedures to ensure fair comparison. Fine-tuning utilized the AdamW optimizer with learning rate scheduling and early stopping based on validation set performance. Complete hyperparameter configurations and training details are provided in Table~\ref{tab:finetuning_config} in the appendices.

\subsection{Domain-Adapted Architecture: POLITICS Model}

We include the current state-of-the-art system for article-level political bias classification, the POLITICS model developed by \citet{liu2022politics}. This system represents a RoBERTa-based encoder enhanced through continued pre-training with carefully designed contrastive learning objectives.

The POLITICS model addresses source leakage through a triplet-loss framework that encourages the learning of ideology-specific rather than source-specific representations. The training procedure employs $\langle$anchor, positive, negative$\rangle$ triplets where:

\begin{itemize}
    \item \textbf{Anchor}: Article $a$ with political label $l \in \{\text{left}, \text{center}, \text{right}\}$
    \item \textbf{Positive}: Article $p$ with identical label $l$ from a different publication source
    \item \textbf{Negative}: Article $n$ with different label $m \neq l$ from any source
\end{itemize}

The triplet loss objective encourages the model to produce representations where ideologically similar articles from different sources are closer in embedding space than ideologically different articles:

\begin{equation}
    \mathcal{L}_{\text{triplet}} = \sum_{\langle a,p,n \rangle \in \mathcal{T}} \max\left( \|f(a) - f(p)\|_2 - \|f(a) - f(n)\|_2 + \delta, 0\right)
    \label{eqn:triplet_loss}
\end{equation}

where $f(\cdot)$ denotes the model's [CLS] token representation, and $\delta$ is a margin hyperparameter. The POLITICS model incorporates both story-level objectives (positive samples discuss the same event) and ideology-level objectives (positive samples share political orientation) during continued pre-training.

\section{Large Language Model Evaluation}
\label{sec:llm-baselines}

The recent emergence of large-scale language models necessitates comprehensive evaluation of their capabilities for political bias classification. We assess five state-of-the-art models from the GPT \citep{openai2024gpt4,openai_o3_systemcard_2025,openai_gpt5_systemcard_2025} and LLaMA families \citep{touvron2023llama} in zero-shot configuration to establish baseline performance without task-specific adaptation.

\subsection{Model Selection and Access}

Our evaluation encompasses models from two prominent LLM families:

\begin{itemize}
    \item \textbf{GPT Family}: GPT-4o-mini, GPT-5-nano, and o3-mini accessed via OpenAI API
    \item \textbf{LLaMA Family}: LLaMA--3.1--8B-instant and LLaMA--3.3--70B-versatile accessed via GroqCloud API
\end{itemize}

LLaMA models were evaluated with temperature = 1.0 to maintain output diversity, while GPT models employed default temperature settings. For computational efficiency, we conducted preliminary screening on a 100-article subset to identify the most promising GPT variants before full-scale evaluation.

\subsection{Prompt Design and Evaluation Protocol}

To ensure consistent evaluation across models, we employed a standardized prompt format designed to elicit structured responses:

\begin{quote}
\texttt{You are a political bias classifier specific to U.S. politics. Given the user's INPUT TEXT, return only valid JSON of the form \{''bias'':''left|center|right''\}. No extra text.}
\end{quote}

This prompt design minimizes variability in response format while providing clear task specification. All models were evaluated exclusively on the Media Split configuration to maintain consistency with our content-based evaluation framework. We also tested multiple prompt variants, including chain-of-thought and detailed definition prompts, but found no significant performance improvements over the baseline prompt. These are detailed in Appendix~\ref{sec:opro}.

\section{Baseline Performance Analysis}
\label{sec:baseline-results}

Table~\ref{tab:main_baseline} presents comprehensive baseline results across all evaluated systems. The performance patterns reveal several important findings regarding the challenges and opportunities in automated political bias classification.

\subsection{Source Leakage Effects}

The comparison between Media Split and Random Split performance provides crucial insights into source leakage susceptibility. All transformer models demonstrate substantially higher performance on the Random Split, with improvements of 30--50 F1 points across most configurations. This performance differential is most pronounced for center-category classification, where Random Split performance exceeds Media Split by over 40 points in several cases.

These results indicate that centrist publications possess distinctive lexical or stylistic signatures that models can easily memorize when source information is available during training. However, the substantial performance degradation on the Media Split suggests that these signatures may not correspond to genuine ideological content markers, highlighting the importance of source-controlled evaluation for political bias detection systems.

\subsection{Transformer Architecture and Large Language Model Performance}

Among general-purpose transformers, RoBERTa achieved the highest Media Split performance (F1-Macro = 42.34), followed by BERT (38.26) and BART (36.01). However, all general-purpose models were substantially outperformed by the domain-adapted POLITICS system (57.66), demonstrating the value of triplet-loss pre-training in learning useful embeddings for this kind of nuanced classification task.

Zero-shot LLM performance varied considerably across model families and sizes. GPT family models achieved competitive results, with o3-mini reaching 56.92 F1-Macroâ€”approaching the performance of the specialized POLITICS system. Notably, LLMs demonstrated relatively stronger performance on center (53.46) and right (70.78) categories compared to transformer baselines.

However, LLaMA family models underperformed significantly, with both evaluated models falling below human baseline performance. This disparity may reflect differences in training data composition, model architecture, or prompt sensitivity across LLM families.

\subsection{Performance Interpretation Challenges}

LLM performance interpretation is complicated by potential training data contamination. Given the massive scale and often proprietary nature of LLM training corpora, we cannot exclude the possibility that these models encountered AllSides articles or similar content during pre-training. This potential exposure could enable memorization of source-specific patterns rather than genuine content-based classification capabilities.

This concern does not apply to our transformer baselines, where training data composition is fully controlled and transparent. Consequently, transformer results on the Media Split provide the most reliable assessment of content-based classification performance.

These baseline findings align with observations by \citet{Wen_2023}, who demonstrate that while LLMs achieve competitive performance on explicit classification tasks, they consistently underperform specialized fine-tuned models on nuanced tasks requiring deeper contextual understanding.

% TODO: - Error analysis across different article types and lengths
% TODO: - Analysis of performance correlation with article metadata (length, topic, etc.)
