We investigate the effectiveness of metalinguistic features for article-level political ideology classification in U.S. news media, comparing domain adapted transformer models with large language models (LLMs). We introduce a multi-objective continued pre-training approach that incorporates article level theme and tone prediction as auxiliary tasks. After fine-tuning on the AllSides dataset \citep{baly2020we}, our best configuration outperforms other baselines and state-of-the-art models on the ideology prediction task. Our experiments also show that domain-adapted transformer architectures maintain superior performance over LLMs even when the latter are fine-tuned with task-specific training data, highlighting the importance of architectural design and training objective formulation for this classification task. We also release an expanded version of the AllSides dataset with over 10,000 additional annotated articles to facilitate future research in political ideology detection.
