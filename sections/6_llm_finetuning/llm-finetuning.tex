\chapter{Large Language Model Fine-tuning for Political Bias Classification}
\label{sec:llm-finetuning}

This chapter investigates the effectiveness of fine-tuning contemporary large language models for political bias classification, attempting to answer whether general-purpose LLMs can match or exceed the performance of specialized transformer architectures when provided with task-specific training data. We conduct systematic experiments with GPT-4o-mini across varying training set sizes and input length configurations, revealing important insights about the scalability and limitations of LLM-based approaches to political ideology detection.

\input{sections/6_llm_finetuning/motivation.tex}
\input{sections/6_llm_finetuning/experimental_design.tex}
\input{sections/6_llm_finetuning/results_analysis.tex}
