\section{Fine-tuned Large Language Models}

\begin{table*}[ht]
\centering
\small
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Specs} & \multicolumn{4}{c}{\textbf{Full Text}} & \multicolumn{4}{c}{\textbf{400 Words}} \\ 
\midrule
 & F1-Macro & Left & Center & Right & F1-Macro & Left & Center & Right \\
 \midrule
baseline & 51.86 & 52.88 & 39.76 & 62.96 & 48.37 & 46.46 & 41.97 & 56.67  \\
3 epochs, 150 samples & 33.37 &  43.77 & 15.44 & 40.90 & 62.88 & 68.89 & 41.07 & 78.72 \\
3 epochs, 300 samples & 25.04 & 58.50 & 05.13 & 11.50 & 56.65 & 57.67 & 32.17 & 80.11 \\
3 epochs, 1000 samples & 69.23 & 70.98 & 52.96 & \textbf{83.76} & 57.21 & 62.13 & 31.61 & \textbf{77.89}  \\
3 epochs, 2000 samples & \textbf{69.37} & \textbf{71.83} & \textbf{53.50} & 82.79 & \textbf{68.85} & \textbf{78.68} & 39.63 & 75.08  \\
10 epochs, 150 samples & 28.03 & 52.75 & 08.93 & 22.41 & 58.14 & 54.40 & \textbf{43.26} & 76.78 \\
\bottomrule
\end{tabular}
\caption{GPT-4o-mini Finetuning Experiments}
\label{tab:llm_finetuning}
\end{table*}


In our baseline experiments, the POLITICS model outperformed LLMs that were pretrained on much larger amounts of data. We explored fine-tuning an LLM on varying amounts of data and input lengths. Based on both performance and cost considerations we selected \texttt{GPT-4o-mini} for these experiments, as it offered the most favorable balance of accuracy and computational efficiency. We performed fine-tuning using the OpenAI API.

\subsection{Results}

Table~\ref{tab:llm_finetuning} presents our results for different amounts of fine-tuning examples. We also experimenting with fine-tuning articles truncated to 400 words. The full-text experiments showed high variability, with F1-Macro scores ranging from 25.04 to 69.37 across different configurations, with at least 1000 samples (69.23) required to show an improvement over the baseline. The best performance (69.37) was achieved with 2000 samples and full text. As this is a minimal increase over the 1000-sample configuration, this suggests that the model may be approaching its performance ceiling with full-text inputs.

The 400-word truncation experiments demonstrated more consistent improvements with fine-tuning compared to full-text experiments, with all truncated configurations outperforming their baseline (maximum F1-Macro of 68.85 with 2000 samples). This finding aligns with recent work by \citet{ibrahim2024analyzingpoliticalstancestwitter}, who achieved a high F1-Macro score of 91.1 when analyzing political stances on Twitter using GPT-4o and Gemini-Opus. Both our results and the work done by \citet{ibrahim2024analyzingpoliticalstancestwitter} suggest that models perform better on shorter political texts, with performance degrading as text length increases. Interestingly, across both our short and full-length text experiments, we observed no clear linear relationship between the amount of fine-tuning data and performance improvement, indicating that simply increasing training data volume is insufficient for optimizing political ideology detection capabilities.

\subsection{Optimization by Prompting}
We also experimented with Optimization by Prompting (OPRO) methods (see details and results in Appendix~\ref{sec:opro}) to optimize prompts for GPT-4o-mini. However, none of the optimized prompts outperformed the baseline prompt, suggesting that for our specific task and dataset the additional complexity introduced by the OPRO prompts did not translate into improved performance. The chain of thought prompt (COT) came closest to matching the baseline performance, suggesting that detailed reasoning steps can be beneficial.