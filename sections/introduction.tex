\chapter{Introduction}

\section{Background and Motivation}

In the current digital landscape, the proliferation of politically biased news content presents substantial challenges for democratic discourse and informed decision-making. The increasing polarization of media sources, combined with algorithmic content curation systems, has created online ecosystems where partipants are exposed to ideologically diverse context, yet are becoming politically polarised \citep{flaxman2016filterbubblesechochambers,kitchens2020echochambersfilterbubbles,Garimella_Smith_Weiss_West_2021, cinelli2021echochambereffectsocialmedia}. As people are generally worse at identifying their own biases \citep{wang2020bias} than those of others, this phenomenon raises concerns about how individuals consume, interpret, and share news information. 

The automatic detection and classification of political bias in textual content has emerged as a recent research area within natural language processing and computational social science. Such systems have practical applications in media literacy tools, content recommendation algorithms, fact-checking platforms, and journalistic analysis workflows. However, the computational modeling of political ideology remains a challenging problem due to the inherent subjectivity of bias perception, the complex relationship between explicit content and implicit framing, and the need to distinguish between source-level and article-level ideological markers.

Traditional approaches to bias detection have primarily focused on source-level classification, where entire news outlets or authors are categorized according to their general political orientation \citep{baly2020acl, darwish2020aaai, ronnback2025biasoutlets}. While this coarse-grained approach provides useful insights for media analysis, it fails to capture the significant variation in political perspective that can occur within individual articles from the same source. Article-level classification represents a more nuanced and practically valuable approach, enabling fine-grained analysis of ideological content and reducing the risk of overgeneralization based on source reputation alone.

\section{Problem Statement and Research Questions}

This thesis addresses the challenge of automatic political ideology classification at the article level, with particular focus on leveraging metalinguistic features to improve classification performance. The research is motivated by four key limitations in existing work: (1) the lack of comprehensive human baseline studies for article-level political bias classification, (2) limited exploration of metalinguistic features for ideology detection, (3) insufficient evaluation of modern large language models on this task, and (4) the absence of causal analysis examining how specific thematic and tonal features influence ideological classification.

Specifically, this work investigates four interconnected research questions:

\begin{enumerate}
    \item \textbf{Human Performance Baseline}: What level of classification accuracy can human annotators achieve when identifying political ideology from article content alone, and how does this baseline inform the evaluation of automated approaches?
    
    \item \textbf{Metalinguistic Feature Integration}: How effectively can article-level metalinguistic features, particularly thematic and tonal characteristics, be incorporated into transformer-based models to improve political bias classification performance?
    
    \item \textbf{Large Language Model Evaluation}: How do contemporary large language models, evaluated in both zero-shot and fine-tuned configurations, compare to specialized transformer architectures designed for political bias detection?
    
    \item \textbf{Causal Analysis of Theme-Tone-Ideology Relationships}: What are the causal effects of specific thematic and tonal features on political ideology classification, and how do automated systems differ from human expert annotators in their sensitivity to these features?
\end{enumerate}

\section{Technical Approach and Methodology}

Our technical approach centers on extending domain-adapted transformer models with metalinguistic auxiliary objectives during continued pre-training. Building upon the established triplet-loss methodology introduced by \citet{baly2020we} and refined by \citet{liu2022politics}, we develop a multi-task learning framework that incorporates theme and tone prediction as auxiliary tasks alongside the primary ideology classification objective.

Our work also addresses the challenge of source leakage, where classifiers learn to exploit source-specific patterns (such as author names, publication styles, or explicit source mentions) rather than content-based ideological markers. By employing contrastive learning techniques and careful data partitioning strategies, our models are trained to focus on genuine linguistic and stylistic indicators of political orientation.

We extend the AllSides dataset \citep{baly2020we}, which provides article-level political bias annotations for U.S. news content, by incorporating additional annotated articles and enriching the dataset with metalinguistic feature annotations derived from the GDELT Project's V2Themes and V2Tone metadata. This enhanced dataset enables systematic evaluation of the contribution of different feature types to classification performance.

\section{Key Contributions}

The primary contributions of this thesis are:

\begin{enumerate}
    \item \textbf{Novel Metalinguistic Integration}: We present the first systematic integration of metalinguistic features (theme and tone) into article-level political bias classification, demonstrating significant performance improvements particularly for challenging center-leaning content.
    
    \item \textbf{Comprehensive LLM Evaluation}: We conduct the first extensive evaluation of state-of-the-art large language models (including GPT-4o, LLaMA-3, and o3-mini) on political bias classification, examining both zero-shot prompting and fine-tuning approaches.
    
    \item \textbf{Enhanced Dataset and Baseline}: We introduce an expanded version of the AllSides dataset with 47\% additional annotated articles and establish an informal human performance baselines through a small annotation study.
    
    \item \textbf{Methodological Advances}: We develop improved continued pre-training techniques that achieve new state-of-the-art performance on political bias classification while maintaining robustness to source leakage.
    
    \item \textbf{Causal Inference Analysis}: We provide the first causal analysis of theme-tone-ideology relationships in political text classification, comparing human expert judgments with automated system outputs to identify systematic biases and feature dependencies in computational approaches.
\end{enumerate}

\section{Thesis Organization}

The remainder of this thesis is organized as follows. Chapter~\ref{sec:relatedwork} surveys related work in computational bias detection, transformer-based text classification, and metalinguistic feature extraction. Chapter~\ref{sec:baselines} establishes experimental baselines through human annotation studies and evaluation of existing transformer and large language model approaches. Chapter~\ref{sec:dataset} describes our extensions to the AllSides dataset and the integration of metalinguistic features. Chapter~\ref{sec:continued-pretraining} details our continued pre-training methodology incorporating metalinguistic auxiliary objectives. Chapter~\ref{sec:llm-finetuning} presents comprehensive experiments with fine-tuned large language models and prompt optimization techniques. Chapter~\ref{ch:causal-inference} provides a causal analysis of theme-tone-ideology relationships, comparing human expert annotations with automated system outputs to identify systematic biases and feature dependencies in computational approaches. Chapter~\ref{ch:discussion} analyzes and discusses the experimental results, comparing the effectiveness of different approaches. Chapter~\ref{ch:limitations-future-work} conclude with directions for future research and acknowledgment of current limitations.

\chapter{Related Work}
\label{sec:relatedwork}

Early approaches in automated ideology classification predominantly formulated the problem as binary classification or focused on identifying extreme partisan viewpoints (hyper-partisan news) \citep{hube2018detecting, semeval2019hyperpartisan, krieger2022daroberta, lin2024inditag}. The literature is distinguished by granularity of analysis: while substantial research has focused on \textit{sentence-level} bias detection \citep{spinde2021mbic,spinde2021babe, menzner2025biasscanner}, identifying specific biased phrases or sentences, these approaches often fail to capture broader contextual or cumulative framing effects across entire articles. Article-level classification approaches, which we adopt in this work, consider the holistic political orientation of texts.

\section{Computational Bias Detection}

The computational detection of political bias in news media has evolved from simple keyword-based approaches to sophisticated neural architectures. Early work in this domain focused primarily on lexical and syntactic features, often employing traditional machine learning methods such as Support Vector Machines and logistic regression \citep{recasens2013weasel}. These approaches, while interpretable, suffered from limited capacity to capture complex semantic relationships and contextual dependencies.

The advent of pre-trained language models has transformed the landscape of bias detection. \citet{baly2020we} introduced the use of BERT for political ideology classification, employing triplet-loss objectives to address the fundamental challenge of source leakage. Their work demonstrated that contrastive learning could encourage models to focus on content-based ideological markers rather than source-identifying patterns. This methodology was subsequently refined by \citet{liu2022politics}, who developed the POLITICS model through large-scale continued pre-training with enhanced contrastive objectives.

Recent work by \citet{ronnback2025biasoutlets} has explored the integration of semantic, syntactic, and metalinguistic features for source-level bias detection, demonstrating the potential value of multi-modal feature representations. However, the systematic integration of such features into article-level classification systems remains underexplored.

\section{Human Annotation and Baseline Studies}

The establishment of human performance baselines represents a critical component of bias detection research, yet comprehensive studies remain limited. The Media Bias Identification Corpus (MBIC) by \citet{spinde2021mbic} utilized crowdworkers to identify biased language, achieving only slight to fair inter-annotator agreement (Fleiss's Kappa $\approx 0.21$). In response to these annotation challenges, the Bias Annotations by Experts (BABE) dataset \citep{spinde2021babe} employed expert annotators specifically trained to recognize media bias, resulting in improved but still modest inter-annotator reliability (Krippendorff's Alpha $\approx 0.39$).

These findings suggest that even expert human annotators face considerable challenges in consistently identifying biased content. Moreover, these prior studies primarily focused on bias detection rather than directional classification. This thesis addresses this gap by introducing an informal layman baseline study for political ideology prediction that explicitly distinguish ideological direction.

\section{Large Language Models in Political Analysis}

The recent emergence of large-scale language models has opened new possibilities for political text analysis. While these models have demonstrated impressive performance across diverse NLP tasks, their application to political bias detection remains understudied. Recent work by \citet{ibrahim2024analyzingpoliticalstancestwitter} has shown promise for political stance detection in social media contexts, but comprehensive evaluation on news article classification has not been conducted.

The potential of prompt engineering and few-shot learning approaches for political classification presents both opportunities and challenges. While these methods can leverage the extensive world knowledge encoded in large language models, they also raise concerns about training data contamination and reproducibility. This thesis provides the first systematic evaluation of modern LLMs on political bias classification, examining both zero-shot and fine-tuning paradigms.


% TODO: - Discussion of ethical considerations and potential societal impact
% TODO: - Comparison table of different bias detection approaches in the literature
% TODO: - Brief overview of the GDELT metadata and its relevance to political analysis
% TODO: - Discussion of the challenges in defining and measuring political bias
% TODO: - Ethical considerations in automated bias detection systems
% TODO: - Comparison with related work in stance detection and sentiment analysis
% TODO: - Discussion of dataset bias and annotation quality issues
